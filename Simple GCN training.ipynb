{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MARA - IMDB_mlh dataset tests - by Bartosz Trojan\n",
    "The implementation will be based on the official MARA paper\n",
    "Right now I don't have much to show, but this notebook will be updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zF5bw3m9UrMy",
    "outputId": "c9d66c9b-16c8-4b53-9e39-ec9f4530ed22"
   },
   "outputs": [],
   "source": [
    "# os.environ['TORCH'] = torch.__version__\n",
    "# print(torch.__version__)\n",
    "\n",
    "# !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "# !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "# !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lewy700/Documents/MARA/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on device:  cuda\n",
      "IMDB movie type dataset:\n",
      " Number of nodes: 5614\n",
      " Number of edges: 14715\n",
      " Number of edges: layer1: 5443, layer2: 3658, cross_layer: 5614\n",
      " Number of features: 1000\n",
      " Number of classes: 3\n",
      " Number of nodes per class: tensor([ 640, 2438, 2536], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "from utils.read_data_new import IMDB_mlh\n",
    "from config import config\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Working on device: \", device)\n",
    "\n",
    "imdb = IMDB_mlh().to(device)\n",
    "imdb.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropEdge(nn.Module):\n",
    "    def __init__(self, simplification_type=\"l-b-l\", p=0.2):\n",
    "        super().__init__()\n",
    "        self.simplification_type = simplification_type\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, edges, layers_lengths):\n",
    "        if(self.simplification_type == \"l-b-l\"):\n",
    "            intra_layers_length = torch.sum(layers_lengths[:-1])\n",
    "            intra_mask = torch.rand(intra_layers_length) > self.p\n",
    "\n",
    "            intra_layers = edges[:,:intra_layers_length]\n",
    "            edges = torch.cat([intra_layers[:,intra_mask], edges[:,intra_layers_length:]], dim=1)\n",
    "\n",
    "            new_layers_lenghts = []\n",
    "            temp = 0\n",
    "            for i in range(len(layers_lengths)-1):\n",
    "                new_layers_lenghts.append(torch.sum(intra_mask[temp:temp + layers_lengths[i]]))\n",
    "                temp += layers_lengths[i]\n",
    "            new_layers_lenghts.append(layers_lengths[-1])\n",
    "\n",
    "            return edges, torch.tensor(new_layers_lenghts)\n",
    "        \n",
    "        if(self.simplification_type == \"multilayer\"):\n",
    "            mask = torch.rand(edges.shape[1]) > self.p\n",
    "            edges = edges[:, mask]\n",
    "\n",
    "            new_layers_lenghts = []\n",
    "            temp = 0\n",
    "            for i in range(len(layers_lengths)):\n",
    "                new_layers_lenghts.append(torch.sum(mask[temp:temp + layers_lengths[i]]))\n",
    "                temp += layers_lengths[i]\n",
    "\n",
    "            return edges, torch.tensor(new_layers_lenghts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AkQAVluLuxT_",
    "outputId": "fec9f0d9-8c17-46e0-a2fa-92c75203c180"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MARA(\n",
      "  (conv1): GCNConv(1000, 512)\n",
      "  (conv2): GCNConv(512, 256)\n",
      "  (conv3): GCNConv(256, 52)\n",
      "  (classifier): Linear(in_features=52, out_features=3, bias=True)\n",
      "  (ReLU): ReLU6()\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (dropedge): DropEdge()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Dlaczego oni nie wspominają o żadnych funkcjach aktywacji w MARZE???\n",
    "\n",
    "class MARA(nn.Module):\n",
    "    def __init__(self, simplificaton_type=config[\"simplification_type\"], simplification_stages=config[\"simplification_stages\"], simplification_strategy=config[\"simplification_strategy\"], DE_p=config[\"DE_p\"], NS_k=config[\"NS_k\"]):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234)\n",
    "        \n",
    "        self.simplification_type = simplificaton_type\n",
    "        self.simplification_stages = simplification_stages\n",
    "        self.simplification_strategy = simplification_strategy\n",
    "        self.DE_p = DE_p\n",
    "        self.NS_k = NS_k\n",
    "        \n",
    "        self.conv1 = GCNConv(imdb.get_number_of_features(), 512)\n",
    "        self.conv2 = GCNConv(512, 256)\n",
    "        self.conv3 = GCNConv(256, 52)\n",
    "        self.classifier = nn.Linear(52, imdb.get_number_of_classes())\n",
    "        self.ReLU = torch.nn.ReLU6()\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(0) # ręcznie sprawdziłem dropout 0.1, 0.2 i 0.3 i żaden nie zwiększa wyniku\n",
    "\n",
    "        self.dropedge = DropEdge(self.simplification_type, self.DE_p)\n",
    "\n",
    "    def forward(self, x, edges, layers_lengths):\n",
    "        if self.simplification_stages == \"once\":\n",
    "            edges, layers_lengths = self.dropedge(edges, layers_lengths)\n",
    "            h = self.conv1(x, edges)\n",
    "            h = self.dropout(h)\n",
    "            h = self.ReLU(h)\n",
    "            h = self.conv2(h, edges)\n",
    "            h = self.dropout(h)\n",
    "            h = self.ReLU(h)\n",
    "            h = self.conv3(h, edges)\n",
    "            h = self.dropout(h)\n",
    "            h = self.ReLU(h)\n",
    "\n",
    "        if self.simplification_stages == \"each\":\n",
    "            edges, layers_lengths = self.dropedge(edges, layers_lengths)\n",
    "            h = self.conv1(x, edges)\n",
    "            h = self.dropout(h)\n",
    "            h = self.ReLU(h)\n",
    "            edges, layers_lengths = self.dropedge(edges, layers_lengths)\n",
    "            h = self.conv2(h, edges)\n",
    "            h = self.dropout(h)\n",
    "            h = self.ReLU(h)\n",
    "            edges, layers_lengths = self.dropedge(edges, layers_lengths)\n",
    "            h = self.conv3(h, edges)\n",
    "            h = self.dropout(h)\n",
    "            h = self.ReLU(h)\n",
    "\n",
    "        out = torch.sigmoid(self.classifier(h))\n",
    "\n",
    "        return out, h\n",
    "\n",
    "model = MARA()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "id": "nwHtX5siwe2v",
    "outputId": "f784ae4a-9cac-4380-cf61-de02029f3117"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5614, 3])\n",
      "torch.Size([5614, 52])\n",
      "tensor(1400, device='cuda:0') tensor(1328, device='cuda:0') tensor(2886, device='cuda:0')\n",
      "torch.Size([5614]) torch.Size([5614]) torch.Size([5614])\n",
      "tensor(1400, device='cuda:0') tensor(1328, device='cuda:0') tensor(2886, device='cuda:0')\n",
      "tensor(700, device='cuda:0') tensor(664, device='cuda:0') tensor(1443, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model = MARA(simplification_stages=\"once\", simplification_strategy=\"multilayer\", DE_p=0.2).to(device)\n",
    "\n",
    "out, h = model(imdb.node_features, torch.cat([imdb.layer_1, imdb.layer_2, imdb.cross_edges], dim=0).t(), torch.tensor([imdb.layer_1.shape[0], imdb.layer_2.shape[0], imdb.cross_edges.shape[0]]))\n",
    "\n",
    "print(out.shape)\n",
    "print(h.shape)\n",
    "\n",
    "# replace mask with masks\n",
    "\n",
    "# test masks for sure\n",
    "tr, val, test = imdb.get_training_mask()\n",
    "\n",
    "print(torch.sum(tr), torch.sum(val), torch.sum(test))\n",
    "print(torch.sum(tr[:2807]), torch.sum(val[:2807]), torch.sum(test[:2807]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DropEdge(torch.autograd.Function):\n",
    "#     def __init__(self, simplification_type=\"l-b-l\", p=0.2):\n",
    "#         self.simplification_type = simplification_type\n",
    "#         self.p = p\n",
    "        \n",
    "#     @staticmethod\n",
    "#     def forward(ctx, intra_edges, cross_edges):\n",
    "#         if(ctx.simplification_type == \"l-b-l\"):\n",
    "#             mask = torch.rand(intra_edges.shape) > ctx.p\n",
    "#             ctx.save_for_backward(mask)\n",
    "\n",
    "#             return intra_edges[mask], cross_edges\n",
    "        \n",
    "#         if(ctx.simplification_type == \"multilayer\"):\n",
    "#             intra_mask = torch.rand(intra_edges.shape) > ctx.p\n",
    "#             cross_mask = torch.rand(cross_edges.shape) > ctx.p\n",
    "#             ctx.save_for_backward(intra_mask, cross_mask)\n",
    "\n",
    "#             return intra_edges[intra_mask], cross_edges[cross_mask]\n",
    "\n",
    "#     @staticmethod\n",
    "#     def backward(ctx, grad_output):\n",
    "#         if(ctx.simplification_type == \"l-b-l\"):\n",
    "#             mask = ctx.saved_tensors\n",
    "            \n",
    "#             return intra_edges[mask], cross_edges\n",
    "        \n",
    "#         if(ctx.simplification_type == \"multilayer\"):\n",
    "#             intra_mask = torch.rand(intra_edges.shape) > ctx.p\n",
    "#             cross_mask = torch.rand(cross_edges.shape) > ctx.p\n",
    "#             ctx.save_for_backward(intra_mask, cross_mask)\n",
    "\n",
    "#             return intra_edges[intra_mask], cross_edges[cross_mask]\n",
    "\n",
    "#         A = grad_output * D\n",
    "#         return A / (1-p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "etxOsz8QIbMO",
    "outputId": "466bf583-72a0-435a-9664-5d2cd6e35b0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1388, device='cuda:0') tensor(1432, device='cuda:0') tensor(2794, device='cuda:0')\n",
      "torch.Size([5614]) torch.Size([5614]) torch.Size([5614])\n",
      "Epoch 5 | Loss: 1.0841 | Train AUC: 0.6932 | Val AUC: 0.6546\n",
      "Epoch 10 | Loss: 1.0035 | Train AUC: 0.6410 | Val AUC: 0.5815\n",
      "Epoch 15 | Loss: 0.9801 | Train AUC: 0.6812 | Val AUC: 0.6241\n",
      "Epoch 20 | Loss: 0.9762 | Train AUC: 0.7846 | Val AUC: 0.7008\n",
      "Epoch 25 | Loss: 0.9626 | Train AUC: 0.8295 | Val AUC: 0.7608\n",
      "Epoch 30 | Loss: 0.9487 | Train AUC: 0.8534 | Val AUC: 0.7810\n",
      "Epoch 35 | Loss: 0.9213 | Train AUC: 0.9342 | Val AUC: 0.8353\n",
      "Epoch 40 | Loss: 0.8692 | Train AUC: 0.9607 | Val AUC: 0.8788\n",
      "Epoch 45 | Loss: 0.7718 | Train AUC: 0.9645 | Val AUC: 0.8642\n",
      "Epoch 50 | Loss: 0.6936 | Train AUC: 0.9755 | Val AUC: 0.8457\n",
      "Epoch 55 | Loss: 0.6639 | Train AUC: 0.9841 | Val AUC: 0.8226\n",
      "Epoch 60 | Loss: 0.6503 | Train AUC: 0.9883 | Val AUC: 0.8083\n",
      "Epoch 65 | Loss: 0.6361 | Train AUC: 0.9912 | Val AUC: 0.8088\n",
      "Epoch 70 | Loss: 0.6212 | Train AUC: 0.9954 | Val AUC: 0.8110\n",
      "Epoch 75 | Loss: 0.6023 | Train AUC: 0.9955 | Val AUC: 0.8154\n",
      "Epoch 80 | Loss: 0.5879 | Train AUC: 0.9949 | Val AUC: 0.8347\n",
      "Epoch 85 | Loss: 0.5792 | Train AUC: 0.9967 | Val AUC: 0.8389\n",
      "Early stopping at epoch 90\n",
      "Final Test AUC: 0.8357 | Whole Dataset AUC: 0.8848\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "model = MARA().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.002, weight_decay=0.0005)\n",
    "\n",
    "train_mask, val_mask, test_mask = imdb.get_training_mask(train_mask_size=0.25, val_mask_size=0.25)\n",
    "\n",
    "def roc_auc(preds, labels):\n",
    "    return roc_auc_score(labels.detach().cpu(), torch.softmax(preds.detach().cpu(), 1), multi_class='ovr')\n",
    "\n",
    "def train(data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    edges = torch.cat([data.layer_1, data.layer_2, data.cross_edges], dim=0).t()\n",
    "    layers_lengths = torch.tensor([data.layer_1.shape[0], data.layer_2.shape[0], data.cross_edges.shape[0]], dtype=torch.int64)\n",
    "    out, h = model(data.node_features, edges, layers_lengths)\n",
    "\n",
    "    train_loss = criterion(out[train_mask], data.classes[train_mask])\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_score = roc_auc(out[train_mask], data.classes[train_mask])\n",
    "    val_score = roc_auc(out[val_mask], data.classes[val_mask])\n",
    "\n",
    "    return train_loss.item(), train_score, val_score\n",
    "\n",
    "def evaluate(data, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        edges = torch.cat([data.layer_1, data.layer_2, data.cross_edges], dim=0).t()\n",
    "        layers_lengths = torch.tensor([data.layer_1.shape[0], data.layer_2.shape[0], data.cross_edges.shape[0]], dtype=torch.int64)\n",
    "        out, h = model(data.node_features, edges, layers_lengths)\n",
    "        score = roc_auc(out[mask], data.classes[mask])\n",
    "    return score\n",
    "\n",
    "# If you want to run this without early stopping, set patience to \n",
    "# bigger number than epoch number\n",
    "best_val_score = 0\n",
    "patience = 50\n",
    "patience_counter = 0\n",
    "best_weights = None\n",
    "\n",
    "for epoch in range(201):\n",
    "    train_loss, train_score, val_score = train(imdb)\n",
    "\n",
    "    if val_score > best_val_score:\n",
    "        best_val_score = val_score\n",
    "        patience_counter = 0\n",
    "        best_weights = model.state_dict()\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1} | Loss: {train_loss:.4f} | Train AUC: {train_score:.4f} | Val AUC: {val_score:.4f}\")\n",
    "\n",
    "model.load_state_dict(best_weights)\n",
    "\n",
    "test_score = evaluate(imdb, test_mask)\n",
    "whole_score = evaluate(imdb, slice(None))\n",
    "print(f\"Final Test AUC: {test_score:.4f} | Whole Dataset AUC: {whole_score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
