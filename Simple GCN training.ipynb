{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MARA - IMDB_mlh dataset tests - by Bartosz Trojan\n",
    "The implementation will be based on the official MARA paper\n",
    "Right now I don't have much to show, but this notebook will be updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zF5bw3m9UrMy",
    "outputId": "c9d66c9b-16c8-4b53-9e39-ec9f4530ed22"
   },
   "outputs": [],
   "source": [
    "# os.environ['TORCH'] = torch.__version__\n",
    "# print(torch.__version__)\n",
    "\n",
    "# !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "# !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "# !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lewy700/Documents/MARA/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB movie type dataset:\n",
      " Number of nodes: 5614\n",
      " Number of edges: 14715\n",
      " Number of edges: layer1: 5443, layer2: 3658, cross_layer: 5614\n",
      " Number of features: 1000\n",
      " Number of classes: 3\n",
      " Number of nodes per class: tensor([ 640, 2438, 2536])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "from utils.read_data_new import IMDB_mlh\n",
    "from config import config\n",
    "\n",
    "imdb = IMDB_mlh()\n",
    "imdb.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropEdge(nn.Module):\n",
    "    def __init__(self, simplification_type=\"l-b-l\", p=0.2):\n",
    "        super().__init__()\n",
    "        self.simplification_type = simplification_type\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, edges, layers_lengths):\n",
    "        if(self.simplification_type == \"l-b-l\"):\n",
    "            intra_layers_length = torch.sum(layers_lengths[:-1])\n",
    "            intra_mask = torch.rand(intra_layers_length) > self.p\n",
    "\n",
    "            intra_layers = edges[:,:intra_layers_length]\n",
    "            edges = torch.cat([intra_layers[:,intra_mask], edges[:,intra_layers_length:]], dim=1)\n",
    "\n",
    "            new_layers_lenghts = []\n",
    "            temp = 0\n",
    "            for i in range(len(layers_lengths)-1):\n",
    "                new_layers_lenghts.append(torch.sum(intra_mask[temp:temp + layers_lengths[i]]))\n",
    "                temp += layers_lengths[i]\n",
    "            new_layers_lenghts.append(layers_lengths[-1])\n",
    "\n",
    "            return edges, torch.tensor(new_layers_lenghts)\n",
    "        \n",
    "        if(self.simplification_type == \"multilayer\"):\n",
    "            mask = torch.rand(edges.shape[1]) > self.p\n",
    "            edges = edges[:, mask]\n",
    "\n",
    "            new_layers_lenghts = []\n",
    "            temp = 0\n",
    "            for i in range(len(layers_lengths)):\n",
    "                new_layers_lenghts.append(torch.sum(mask[temp:temp + layers_lengths[i]]))\n",
    "                temp += layers_lengths[i]\n",
    "\n",
    "            return edges, torch.tensor(new_layers_lenghts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AkQAVluLuxT_",
    "outputId": "fec9f0d9-8c17-46e0-a2fa-92c75203c180"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MARA(\n",
      "  (conv1): GCNConv(1000, 512)\n",
      "  (conv2): GCNConv(512, 256)\n",
      "  (conv3): GCNConv(256, 52)\n",
      "  (classifier): Linear(in_features=52, out_features=3, bias=True)\n",
      "  (dropedge): DropEdge()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Dlaczego oni nie wspominają o żadnych funkcjach aktywacji w MARZE???\n",
    "\n",
    "class MARA(nn.Module):\n",
    "    def __init__(self, simplificaton_type=config[\"simplification_type\"], simplification_stages=config[\"simplification_stages\"], simplification_strategy=config[\"simplification_strategy\"], DE_p=config[\"DE_p\"], NS_k=config[\"NS_k\"]):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234)\n",
    "        \n",
    "        self.simplification_type = simplificaton_type\n",
    "        self.simplification_stages = simplification_stages\n",
    "        self.simplification_strategy = simplification_strategy\n",
    "        self.DE_p = DE_p\n",
    "        self.NS_k = NS_k\n",
    "        \n",
    "        self.conv1 = GCNConv(imdb.get_number_of_features(), 512)\n",
    "        self.conv2 = GCNConv(512, 256)\n",
    "        self.conv3 = GCNConv(256, 52)\n",
    "        self.classifier = nn.Linear(52, imdb.get_number_of_classes())\n",
    "\n",
    "        self.dropedge = DropEdge(self.simplification_type, self.DE_p)\n",
    "\n",
    "    def forward(self, x, edges, layers_lengths):\n",
    "        if self.simplification_stages == \"once\":\n",
    "            edges, layers_lengths = self.dropedge(edges, layers_lengths)\n",
    "            h = self.conv1(x, edges)\n",
    "            h = h.tanh()\n",
    "            h = self.conv2(h, edges)\n",
    "            h = h.tanh()\n",
    "            h = self.conv3(h, edges)\n",
    "            h = h.tanh()\n",
    "\n",
    "        if self.simplification_stages == \"each\":\n",
    "            edges, layers_lengths = self.dropedge(edges, layers_lengths)\n",
    "            h = self.conv1(x, edges)\n",
    "            h = h.tanh()\n",
    "            edges, layers_lengths = self.dropedge(edges, layers_lengths)\n",
    "            h = self.conv2(h, edges)\n",
    "            h = h.tanh()\n",
    "            edges, layers_lengths = self.dropedge(edges, layers_lengths)\n",
    "            h = self.conv3(h, edges)\n",
    "            h = h.tanh()\n",
    "\n",
    "        out = torch.sigmoid(self.classifier(h))\n",
    "\n",
    "        return out, h\n",
    "\n",
    "model = MARA()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "id": "nwHtX5siwe2v",
    "outputId": "f784ae4a-9cac-4380-cf61-de02029f3117"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5614, 3])\n",
      "torch.Size([5614, 52])\n"
     ]
    }
   ],
   "source": [
    "model = MARA(simplification_stages=\"each\", simplification_strategy=\"l-b-l\", DE_p=0.2)\n",
    "\n",
    "out, h = model(imdb.node_features, torch.cat([imdb.layer_1, imdb.layer_2, imdb.cross_edges], dim=0).t(), torch.tensor([imdb.layer_1.shape[0], imdb.layer_2.shape[0], imdb.cross_edges.shape[0]]))\n",
    "\n",
    "print(out.shape)\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DropEdge(torch.autograd.Function):\n",
    "#     def __init__(self, simplification_type=\"l-b-l\", p=0.2):\n",
    "#         self.simplification_type = simplification_type\n",
    "#         self.p = p\n",
    "        \n",
    "#     @staticmethod\n",
    "#     def forward(ctx, intra_edges, cross_edges):\n",
    "#         if(ctx.simplification_type == \"l-b-l\"):\n",
    "#             mask = torch.rand(intra_edges.shape) > ctx.p\n",
    "#             ctx.save_for_backward(mask)\n",
    "\n",
    "#             return intra_edges[mask], cross_edges\n",
    "        \n",
    "#         if(ctx.simplification_type == \"multilayer\"):\n",
    "#             intra_mask = torch.rand(intra_edges.shape) > ctx.p\n",
    "#             cross_mask = torch.rand(cross_edges.shape) > ctx.p\n",
    "#             ctx.save_for_backward(intra_mask, cross_mask)\n",
    "\n",
    "#             return intra_edges[intra_mask], cross_edges[cross_mask]\n",
    "\n",
    "#     @staticmethod\n",
    "#     def backward(ctx, grad_output):\n",
    "#         if(ctx.simplification_type == \"l-b-l\"):\n",
    "#             mask = ctx.saved_tensors\n",
    "            \n",
    "#             return intra_edges[mask], cross_edges\n",
    "        \n",
    "#         if(ctx.simplification_type == \"multilayer\"):\n",
    "#             intra_mask = torch.rand(intra_edges.shape) > ctx.p\n",
    "#             cross_mask = torch.rand(cross_edges.shape) > ctx.p\n",
    "#             ctx.save_for_backward(intra_mask, cross_mask)\n",
    "\n",
    "#             return intra_edges[intra_mask], cross_edges[cross_mask]\n",
    "\n",
    "#         A = grad_output * D\n",
    "#         return A / (1-p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "etxOsz8QIbMO",
    "outputId": "466bf583-72a0-435a-9664-5d2cd6e35b0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training mask: 1388/5614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========  10  ========\n",
      "Loss: 0.7244137525558472\n",
      "Train accuracy: 0.8386167287826538\n",
      "Test accuracy: 0.7084713578224182\n",
      "========  20  ========\n",
      "Loss: 0.6256277561187744\n",
      "Train accuracy: 0.9056196212768555\n",
      "Test accuracy: 0.712257444858551\n",
      "========  30  ========\n",
      "Loss: 0.6149313449859619\n",
      "Train accuracy: 0.960374653339386\n",
      "Test accuracy: 0.6997160315513611\n",
      "========  40  ========\n",
      "Loss: 0.5956122279167175\n",
      "Train accuracy: 0.9668588042259216\n",
      "Test accuracy: 0.6876478791236877\n",
      "========  50  ========\n",
      "Loss: 0.5858041048049927\n",
      "Train accuracy: 0.9711815714836121\n",
      "Test accuracy: 0.693327009677887\n",
      "========  60  ========\n",
      "Loss: 0.5789570212364197\n",
      "Train accuracy: 0.9776657223701477\n",
      "Test accuracy: 0.6836251616477966\n",
      "========  70  ========\n",
      "Loss: 0.5722885131835938\n",
      "Train accuracy: 0.9812679886817932\n",
      "Test accuracy: 0.6798390746116638\n",
      "========  80  ========\n",
      "Loss: 0.5712034702301025\n",
      "Train accuracy: 0.9827089309692383\n",
      "Test accuracy: 0.671320378780365\n",
      "========  90  ========\n",
      "Loss: 0.5694078803062439\n",
      "Train accuracy: 0.9834293723106384\n",
      "Test accuracy: 0.6701372265815735\n",
      "========  100  ========\n",
      "Loss: 0.5662857890129089\n",
      "Train accuracy: 0.9855907559394836\n",
      "Test accuracy: 0.666587769985199\n",
      "========  110  ========\n",
      "Loss: 0.5652025938034058\n",
      "Train accuracy: 0.9877521395683289\n",
      "Test accuracy: 0.6606720089912415\n",
      "========  120  ========\n",
      "Loss: 0.5655543208122253\n",
      "Train accuracy: 0.9863112568855286\n",
      "Test accuracy: 0.6637482047080994\n",
      "========  130  ========\n",
      "Loss: 0.5638132691383362\n",
      "Train accuracy: 0.9891930818557739\n",
      "Test accuracy: 0.6651679873466492\n",
      "========  140  ========\n",
      "Loss: 0.5662026405334473\n",
      "Train accuracy: 0.9863112568855286\n",
      "Test accuracy: 0.6722669005393982\n",
      "========  150  ========\n",
      "Loss: 0.5655746459960938\n",
      "Train accuracy: 0.9855907559394836\n",
      "Test accuracy: 0.6552295088768005\n",
      "========  160  ========\n",
      "Loss: 0.5652697086334229\n",
      "Train accuracy: 0.9855907559394836\n",
      "Test accuracy: 0.6495503783226013\n",
      "========  170  ========\n",
      "Loss: 0.5641825199127197\n",
      "Train accuracy: 0.9884726405143738\n",
      "Test accuracy: 0.6590155959129333\n",
      "========  180  ========\n",
      "Loss: 0.5627149343490601\n",
      "Train accuracy: 0.9899135231971741\n",
      "Test accuracy: 0.6571225523948669\n",
      "========  190  ========\n",
      "Loss: 0.5642680525779724\n",
      "Train accuracy: 0.9870316982269287\n",
      "Test accuracy: 0.6488404870033264\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss, acc, test_acc\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m201\u001b[39m):\n\u001b[0;32m---> 31\u001b[0m     loss, acc, test_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimdb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m======== \u001b[39m\u001b[38;5;124m\"\u001b[39m,epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ========\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 25\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out[train_mask], data\u001b[38;5;241m.\u001b[39mclasses[train_mask])\n\u001b[1;32m     23\u001b[0m acc \u001b[38;5;241m=\u001b[39m accuracy(out[train_mask], data\u001b[38;5;241m.\u001b[39mclasses[train_mask])\n\u001b[0;32m---> 25\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss, acc, test_acc\n",
      "File \u001b[0;32m~/Documents/MARA/.venv/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MARA/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MARA/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = MARA()\n",
    "criterion = torch.nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01) \n",
    "\n",
    "def accuracy(preds, labels):\n",
    "    predicted_labels = torch.argmax(preds, dim=1)\n",
    "    accuracy = (predicted_labels == labels).float().mean()\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "train_mask = imdb.get_training_mask(mask_size=0.25)\n",
    "print(f\"training mask: {torch.sum(train_mask)}/{imdb.get_number_of_nodes()}\")\n",
    "\n",
    "def train(data):\n",
    "    optimizer.zero_grad()\n",
    "    edges = torch.cat([data.layer_1, data.layer_2, data.cross_edges], dim=0).t()\n",
    "    layers_lengths = torch.tensor([data.layer_1.shape[0], data.layer_2.shape[0], data.cross_edges.shape[0]], dtype=torch.int64)\n",
    "    out, h = model(data.node_features, edges, layers_lengths) \n",
    "\n",
    "    test_acc = accuracy(out[train_mask == False], imdb.classes[train_mask == False])\n",
    "\n",
    "    loss = criterion(out[train_mask], data.classes[train_mask])\n",
    "    acc = accuracy(out[train_mask], data.classes[train_mask])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss, acc, test_acc\n",
    "\n",
    "for epoch in range(201):\n",
    "    loss, acc, test_acc = train(imdb)\n",
    "    if (epoch+1)%10 == 0:\n",
    "        print(\"======== \",epoch+1,\" ========\")\n",
    "        print(f\"Loss: {loss}\")\n",
    "        print(f\"Train accuracy: {acc}\")\n",
    "        print(f\"Test accuracy: {test_acc}\")\n",
    "data = imdb\n",
    "edges = torch.cat([data.layer_1, data.layer_2, data.cross_edges], dim=0).t()\n",
    "layers_lengths = torch.tensor([data.layer_1.shape[0], data.layer_2.shape[0], data.cross_edges.shape[0]], dtype=torch.int64)\n",
    "out, h = model(data.node_features, edges, layers_lengths) \n",
    "\n",
    "print(f\"Final accuracy - whole dataset: {accuracy(out, data.classes)}, test_set: {accuracy(out[train_mask == False], data.classes[train_mask == False])}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
